import os
from glob import glob
from os.path import join as pjoin
import Bio
from tqdm import tqdm

workdir : "/crex/proj/uppstore2018116/moritz6/"


HOME = "/crex/proj/uppstore2018116/moritz6/"
TEMP_DIR = '/scratch/'
TRIMMOMATIC_HOME = '/sw/apps/bioinfo/trimmomatic/0.36/rackham/'
COAS_FILES_DIR = pjoin(HOME,'0000_raws/0200_coasses/')
TRIMMOMATIC_JAR_PROGRAM = "trimmomatic.jar"
THREADS = 20
BIN_MAPPING_LIBS = 20
BIN_MAP_MIN = 15
MAX_MEM = '128g'
sortrna_db  = ['rfam-5.8s-database-id98', 'rfam-5s-database-id98', 'silva-arc-16s-id95', 'silva-arc-23s-id98', 'silva-bac-16s-id90', 'silva-bac-23s-id98', 'silva-euk-18s-id95', 'silva-euk-28s-id98']
sortrna_db_loc = '/sw/apps/bioinfo/SortMeRNA/2.1b/rackham/sortmerna'
" ".join([ pjoin(sortrna_db_loc,'rRNA_databases',d+".fasta") + "," + pjoin(sortrna_db_loc,'index',d) for d in sortrna_db])
CLEAN_BINNING = True

trimmomatic_config = {
    	'options' : "-phred33",
        'processing_options' : "ILLUMINACLIP:" + TRIMMOMATIC_HOME + "/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36",
        'cmd' : "java -Xmx" + MAX_MEM + " -Djava.io.tmpdir=" + TEMP_DIR +  " -jar " + os.path.join(TRIMMOMATIC_HOME, TRIMMOMATIC_JAR_PROGRAM) + " PE ",
}

def all_bams(wildcards):
    samples = all_samples()
    path = "{path}/mapping/bams/".format(path = wildcards.path)
    return [pjoin(path,s + ".bam") for s in samples]



def get_coas_libs(wildcards):
    coas_name = wildcards.name
    with open(pjoin(COAS_FILES_DIR, coas_name + ".txt")) as handle:
        samples = [l.strip() for l in handle]
    fwds = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz".format(sample = s) for s in samples]
    revs = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz".format(sample = s) for s in samples]
    u_fwds = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz".format(sample = s) for s in samples]
    u_revs = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz".format(sample = s) for s in samples]
    return {'fwd' : fwds, 'rev' : revs, 'u_rev' : u_revs, 'u_fwd' : u_fwds}

def find_fastq(wildcards):
    path = '0000_raws/0100_reads/genomic/'
    result = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.fastq.gz')) if "/" + wildcards.sample +"_" in y]
    assert len(result) == 2, print(result)
    return result

def all_samples():
    path = "1000_processed_reads/"
    samples = [d for d in os.listdir(path) if os.path.isdir(pjoin(path,d)) ]
    return samples

def all_dones(wildcards):
    path = '0000_raws/0100_reads/genomic/'
    result = [pjoin("1000_processed_reads/","_".join(s.split("_")[:-4]),"done") for s in os.listdir(path) if s.endswith(".fastq.gz")  ]
    return list(set(result))

def all_mags(wildcards):
    mag_folder = "{path}/{name}/{assembler}/MAGs/".format(path = wildcards.path, name=wildcards.name, assembler = wildcards.assembler)
    name = wildcards.name
    bins = [f.split("-")[-1][:-3] for f in os.listdir(mag_folder) if f.endswith(".fa")]
    results = [mag_folder + "metabat_{name}_{bin}/{name}_{bin}.checkm".format(name=name, bin = f)  for f in bins]

    return results


def all_bin_samples(wildcards):
    path = "{path}/mapping/".format(path = wildcards.path)
    folds = [i for i,v in  enumerate(path.split("/")) if v == "1500_assemblies"]
    if len(folds) == 1:
        coas_name = path.split("/")[folds[0]+1]
        with open(pjoin(COAS_FILES_DIR, coas_name + ".txt")) as handle:
            samples_from_coas = [l.strip() for l in handle]
    else :
        samples_from_coas = []
    rates_file = pjoin(path,"mapping_rates.txt")
    with open( rates_file ) as  handle:
        rates = {l.split()[0] : float(l.split()[1]) for l in handle.readlines()[1:] if float(l.split()[1]) == float(l.split()[1])}
    vvs = sorted(list(rates.values()),reverse = True)
    cutoff = vvs[-1] if len(vvs) < BIN_MAPPING_LIBS else vvs[BIN_MAPPING_LIBS]
    cutoff = cutoff if cutoff > BIN_MAP_MIN else BIN_MAP_MIN
    samples = [k for k, v in rates.items() if v > cutoff]
    samples = list(set(samples_from_coas).union(samples))
    if CLEAN_BINNING and len(samples_from_coas) > 1:
        samples = samples_from_coas
    return [pjoin(pjoin(path,"bams",s + ".bam")) for s in samples]


rule fastqc:
    input :  find_fastq
    output : "1000_processed_reads/{sample}/reads/fastqc"
    threads : THREADS
    shell:
        """
        fastqc -t {threads} -o {output} {input}
        """


rule trimmomatic:
    """ QCing and cleaning reads """
    params : cmd = trimmomatic_config['cmd'],
             options = trimmomatic_config['options'],
             processing_options = trimmomatic_config['processing_options'],
             temp_folder = TEMP_DIR
    input :  find_fastq
    output : read1 = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
             read2 = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz",
             read1U = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz",
             read2U = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz",
    threads : THREADS
    log : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}.log"
    shell:
        """
        unpigz -c `echo {input} | tr ' ' '\n' | grep "_R1_"`  >  {params.temp_folder}/temp_R1.fastq
        unpigz -c `echo {input} | tr ' ' '\n' | grep "_R2_"` >  {params.temp_folder}/temp_R2.fastq

        {params.cmd} {params.options} {params.temp_folder}/temp_R1.fastq {params.temp_folder}/temp_R2.fastq -threads {threads} -baseout {params.temp_folder}/{wildcards.sample}.fastq.gz {params.processing_options} 2> {log}
        mv {params.temp_folder}/{wildcards.sample}*.fastq.gz 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/
        """

rule mash:
    params : kmer = 21,
             hashes = 1000
    input : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz","1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz"
    output : "1000_processed_reads/{sample}/reads/mash/{sample}.msh"
    log : "1000_processed_reads/{sample}/reads/mash/{sample}.log"
    threads : THREADS
    shell :
        "mash sketch -r -p {threads} -k {params.kmer} -s {params.hashes} -o $(echo '{output}' | sed -e 's/.msh//') {input} > {log}"

rule kaiju:
    params : db_path = "/crex2/proj/sllstore2017039/2017_MG_course/data/kaijudb/kaiju_nr_db/",
             db = "kaijudb.fmi"
    input : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz","1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz"
    output : "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.out.summary", "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.html"
    log : "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.log"
    threads : THREADS
    shell : """
    module load bioinfo-tools
    module load Krona
    kaiju -t {params.db_path}/nodes.dmp -f {params.db_path}/{params.db}   -i 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/{wildcards.sample}_1P.fastq.gz -j 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/{wildcards.sample}_2P.fastq.gz -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -z {threads} > {log}
    kaiju2krona -u -t {params.db_path}/nodes.dmp -n {params.db_path}/names.dmp -i 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.krona >> {log}
    ktImportText  -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.html 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.krona >> {log}
    kaijuReport -p -r genus -t {params.db_path}/nodes.dmp -n {params.db_path}/names.dmp -i 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -r family -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.summary >> {log}
    """


rule MAG_stats:
    input : bin_file = "{path}/{name}/{assembler}/MAGs/metabat_{name}_unbinned/{name}_unbinned.checkm",
    output : "{path}/{name}/{assembler}/MAGs/{name}.magstats"
    threads : 1
    run :
        from Bio import SeqIO
        from tqdm import tqdm
        from pandas import DataFrame

        out_dict = {}
        mag_folder = "/".join(input.bin_file.split("/")[:-2])
        mag_folder = "{path}/{name}/{assembler}/MAGs/".format(path = wildcards.path, name=wildcards.name, assembler = wildcards.assembler)
        bins = [f.split("-")[-1][:-3] for f in os.listdir(mag_folder) if f.endswith(".fa")]
        bin_files = [mag_folder + "metabat_{name}_{bin}/{name}_{bin}.checkm".format(name=wildcards.name, bin = f)  for f in bins]

        out_file = output[0]

#        with open("phylophlan_tax.txt") as handle:
#            tax = [l.split()[:2] for l in handle.readlines()]
#            tax = {t[0] : ":".join([tt for tt in t[1].split(".") if "?" not in tt ]) for t in tax if "all_samples-" in t[0]}
        def process_bin(binl) :
            bin_head = binl[:-7]
            bin_checkm = bin_head + ".checkm"
            bin_genome= bin_head + ".fna"
            bin_proteom = bin_head + ".faa"
            bin_id = bin_head.split("/")[-1]
            out_dict = {}

            if "unbinned" not in bin_id:
                checkm_fields = ['Bin Id', 'Marker lineage', 'UID', '# genomes', '# markers', '# marker sets', '0', '1', '2', '3', '4', '5+', 'Completeness', 'Contamination', 'Strain heterogeneity']
                with open(bin_checkm) as handle:
                    dat = handle.readlines()
                dat = {l.split()[0] : {k : v for k,v in zip(checkm_fields, l[:-1].split() ) } for l in dat[3:-1]}
                dat = dat[bin_id]

                out_dict['completeness'] = float(dat['Completeness'])
                out_dict['contamination'] = float(dat['Contamination'])
                out_dict['taxo:checkm'] = dat['Marker lineage']
                out_dict['strain_heterogeneity'] = float(dat['Strain heterogeneity'])
            else :
                out_dict['completeness'] = None
                out_dict['contamination'] = None
                out_dict['taxo:checkm'] = 'Unbinned'
                out_dict['strain_heterogeneity'] = None


            with open( bin_genome) as handle:
                fna = [s for s in SeqIO.parse(handle, "fasta")]
            with open( bin_proteom) as handle:
                faa = [s for s in SeqIO.parse(handle, "fasta")]

            out_dict['length'] = sum([len(s.seq) for s in fna])
            out_dict['nb_contigs'] = len(fna)
            out_dict['nb_proteins'] = len(faa)
            out_dict['coding_density'] = (3.0*sum([len(s.seq) for s in faa]))/sum([len(s.seq) for s in fna])
            out_dict['GC'] = float(sum([str(s.seq).count("G")+str(s.seq).count("C") for s in fna]))/out_dict['length']
            #out_dict['taxo:phylophlan'] = tax[bin_id]
            return (bin_id, out_dict)

        dat = dict([process_bin(p) for p in tqdm(bin_files)])
        DataFrame.from_dict(dat, orient = 'index').to_csv(out_file)


rule clean_metabat:
    input : "{path}/{name}/{assembler}/mapping/metabat/metabat_{name}"
    output : "{path}/{name}/{assembler}/MAGs/metabat_{name}-unbinned.fa"
    run :
        import os
        from Bio import SeqIO
        from os.path import join as pjoin
        from tqdm import tqdm

        ipath = pjoin(os.path.dirname(input[0]))
	opath = os.path.dirname(output[0])	
	if not os.path.exists(opath):
            os.makedirs(opath)
        for f in tqdm(os.listdir(ipath)):
            if f[-3:] == ".fa":
                with open(pjoin(ipath, f)) as handle:
                    seqs = [s for s in SeqIO.parse(handle, "fasta")]
                zeros = len(str(len(seqs)))
                bin_name = f[:-3]
                for i,s in enumerate(seqs):
                    s.id = bin_name.replace(".","-") + "-" + str(i+1).zfill(zeros)
                    s.description = ""
                with open(pjoin(opath, f[:-3].replace(".","-")+".fa"), "w") as handle:
                    SeqIO.write(seqs, handle, "fasta")

rule all_kaiju:
    input : "1000_processed_reads/done"
    output : "1000_processed_reads/kaiju_table.tsv"
    run :
        from ete3 import NCBITaxa
        from collections import defaultdict
        import os
        from tqdm import tqdm
        from os.path import join as pjoin
        from pandas import DataFrame, concat

        out_file="1000_processed_reads/kaiju_table.tsv"

        pat = "/crex/proj/uppstore2018116/moritz6/1000_processed_reads/"
        samples = [s for s in os.listdir(pat) if "." not in s]
        out_dict = {}

        for s in tqdm(samples):
            if not out_dict.get(s):
                out_dict[s]=defaultdict(int)
                with open(pjoin(pat, s, "reads", "kaiju", s+"_kaiju.out")) as handle:
                    for l in tqdm(handle):
                        out_dict[s][l.split()[2]]+=1

        data = DataFrame.from_dict(out_dict)
        data = data.fillna(0)

        taxDb = NCBITaxa()
        line= {i : taxDb.get_lineage(i) for i in tqdm(list(data.index))}
        out = {i : taxDb.get_taxid_translator(l) if l else None for i,l in tqdm(line.items())}
        tt = sum([list(l.keys()) for l in tqdm(out.values()) if l], [])
        tt = list(set(tt))
        tt = taxDb.get_rank(tt)

        out_l = {k : {tt[k] : v for k,v in t.items()} if t else None for k,t in tqdm(out.items())}
        taxos = DataFrame.from_dict(out_l).transpose()
        taxos = taxos.fillna("NA")
        all = concat([data, taxos.loc[data.index]], axis=1)
	all.to_csv(out_file)

rule megahit_single:
    params : temp_folder = TEMP_DIR
    input : fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
            rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz",
            u_rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz",
            u_fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz"
    output : assembly = "1000_processed_reads/{sample}/megahit/{sample}.fna",
             folder = "1000_processed_reads/{sample}/megahit/data",
             foldermain = "1000_processed_reads/{sample}/megahit"
    threads : THREADS
    shell : """
        unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
        unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
        unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
        unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        megahit  -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -r {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data --out-prefix megahit --continue
        rm -r {params.temp_folder}/temp_data/intermediate_contigs/
        mv {params.temp_folder}/temp_data/ {output.folder}
        cp 1000_processed_reads/{wildcards.sample}/megahit/data/megahit.contigs.fa {output.assembly}
    """

rule spades_single:
    params : temp_folder = TEMP_DIR
    input : fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
            rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz",
            u_rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz",
            u_fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz"
    output : assembly = "1000_processed_reads/{sample}/spades/{sample}.fna",
             folder = "1000_processed_reads/{sample}/spades/data",
             foldermain = "1000_processed_reads/{sample}/spades"
    threads : THREADS
    shell : """
        unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
        unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
        unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
        unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        spades.py --meta -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -s {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data
        mv {params.temp_folder}/temp_data/ {output.folder}
        cp 1000_processed_reads/{wildcards.sample}/spades/data/scaffolds.fasta {output.assembly}
    """


rule megahit_coas:
    params : temp_folder = TEMP_DIR + "/megahit_{name}/"
    input : unpack(get_coas_libs)
    output :    folder = "1500_assemblies/{name}/megahit/data/",
                assembly = "1500_assemblies/{name}/megahit/{name}.fna",
                out = "1500_assemblies/{name}/megahit"
    threads : THREADS
    shell : """
    	 mkdir -p {params.temp_folder}
         unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
         unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
         unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
         unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        megahit  -m 0.8 -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -r {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data --out-prefix megahit --continue
        rm -r {params.temp_folder}/temp_data/intermediate_contigs/
        mv {params.temp_folder}/temp_data/* {output.folder}
        cp 1500_assemblies/{wildcards.name}/megahit/data/megahit.contigs.fa {output.assembly}
    """

rule spades_coas:
    params : temp_folder = TEMP_DIR
    input : unpack(get_coas_libs)
    output :    folder = "1500_assemblies/{name}/spades/data/",
                assembly = "1500_assemblies/{name}/spades/{name}.fna",
                path = "1500_assemblies/{name}/spades"
    threads : THREADS
    shell : """
         unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
         unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
         unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
         unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        spades.py --meta -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -s {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data

        mv {params.temp_folder}/temp_data/* {output.folder}
        cp 1000_processed_reads/{wildcards.name}/spades/data/scaffolds.fasta {output.assembly}

    """



rule bbmap_index:
    input : "{path}/{name}/{assembler}/{name}.fna"
    output : folder = "{path}/{name}/{assembler}/mapping",
             gz = "{path}/{name}/{assembler}/mapping/ref/genome/1/chr1.chrom.gz"
    shell : """
    module load bioinfo-tools
    module load bbmap

    bbmap.sh ref={input} path={output.folder}
    """

rule samnple_wise_bbmap :
    input : index = "{path}/mapping/ref/genome/1/chr1.chrom.gz",
            fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
            rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz"
    output : bam = "{path}/mapping/bams/{sample}.bam",
#             wdups_stats = "{path}/mapping/bams/{sample}_sorted.stats",
#             stats = "{path}/mapping/bams/{sample}.stats"
    threads : THREADS
    shell : """
        module load bioinfo-tools
        module load bbmap
        module load samtools

        home=`pwd`
        cd `dirname {input.index}`/../../../

        bbmap.sh  in=$home/{input.fwd} in2=$home/{input.rev} threads={threads} bamscript=/scratch/{wildcards.sample}.sh out=/scratch/{wildcards.sample}.sam

        /scratch/{wildcards.sample}.sh
        sambamba flagstat -t {threads} /scratch/{wildcards.sample}_sorted.bam > $home/{wildcards.sample}_sorted.stats
        samtools rmdup  /scratch/{wildcards.sample}_sorted.bam /scratch/{wildcards.sample}.bam
        samtools index /scratch/{wildcards.sample}.bam
        sambamba flagstat  -t {threads} /scratch/{wildcards.sample}.bam > $home/{wildcards.sample}.stats

        rm /scratch/{wildcards.sample}.sam
        rm /scratch/{wildcards.sample}_sorted.bam*
        mv /scratch/{wildcards.sample}.bam* bams/
    """


#rule summrize_bbmap:
#    input

rule bbmap_all_samples:
    input : "{path}/mapping/ref/genome/1/chr1.chrom.gz", all_bams
    output : "{path}/mapping/map_table.tsv",
    threads : 1
    shell : """
    jgi_summarize_bam_contig_depths --outputDepth {input[0]}/map_table.tsv  --pairedContigs {input[0]}/paired_contigs.tsv  {input[0]}/bams/*.bam
    """

rule bbmap_binning_samples:
    params : home = pjoin(HOME,"1000_processed_reads"),
    input : index = "{path}/mapping/ref/genome/1/chr1.chrom.gz",
    	    path = "{path}/mapping/",
            bams = all_bin_samples
    output : "{path}/mapping/binmap_table.tsv",
    threads : 1
    shell : """
    jgi_summarize_bam_contig_depths --outputDepth {input.path}/binmap_table.tsv  --pairedContigs {input.path}/binpaired_contigs.tsv {input.bams}
    """


rule bbmap_diagnostic:
    params : home = "/crex/proj/uppstore2018116/moritz6/1000_processed_reads",
             reads = 100000
    input : "{path}/mapping/ref/genome/1/chr1.chrom.gz"
    output : "{path}/mapping/mapping_rates.txt",
    threads : THREADS
    shell : """
        module load bioinfo-tools
        module load bbmap
        module load samtools

        home=`pwd`
        cd {wildcards.path}/mapping

        echo $'sample\tmated\tfwd\trev' > $home/{output}

        for s in `echo """ + " ".join(all_samples()) + """ | tr ' ' "\n"`
        do
            echo mapping $s to {wildcards.path}
            base={params.home}/$s/reads/trimmomatic/$s
            bbmap.sh -Xmx128g  in=${{base}}_1P.fastq.gz in2=${{base}}_2P.fastq.gz threads={threads} out=/dev/null reads={params.reads} 2> tmp
            echo -n $s $'\t' >>  $home/{output}
            cat tmp | grep -E "mated|mapped" | cut -f2  | tr -d ' ' | tr -d % | tr '\n'  '\t' >>  $home/{output}
            echo >> $home/{output}
            rm tmp
        done
    """

rule metabat :
    params : min_len = 1500,
             min_bin_size = 10000,
             maxP = 93,
             minS = 50
    input : mapping = "{path}/{name}/{assembler}/mapping/binmap_table.tsv"
    output : file = "{path}/{name}/{assembler}/mapping/metabat/metabat_{name}"
    threads : THREADS
    shell : """
    metabat2 --maxP {params.maxP} --minS {params.minS} -m {params.min_len}  -s {params.min_bin_size} -i  {wildcards.path}/{wildcards.name}/{wildcards.assembler}/{wildcards.name}.fna -o {output.file} -a {input.mapping}  --saveCls  --unbinned -t {threads}
    """

rule concoct :
    params : min_len = 1500,
             min_bin_size = 10000,
             maxP = 93,
             minS = 50
    input : mapping = "{path}/{name}/{assembler}/mapping/binmap_table.tsv"
    output : file = "{path}/{name}/{assembler}/mapping/concoct/concoct_{name}",
             concoct_abundances = "{path}/{name}/{assembler}/mapping/concoct/concoct_{name}.tsv"
    threads : THREADS
    shell : """
    columns=`head -n1 {input.mapping} | sed 's/\t/\n/g' | grep -n bam | grep -v var | cut -f1 -d":" | sed 's/$/,/' | tr -d '\n'`
    cut -f1,${columns%%,} -d$'\t' {input.mapping} > {output.concoct_abundances}

    """

rule maxbin :
    params : max_exec = "/home/moritz/share/MaxBin-2.2.5/run_MaxBin.pl"
    input : mapping = "{path}/{name}/{assembler}/mapping/binmap_table.tsv",
            assembly = "{path}/{name}/{assembler}/{name}.fna"
    output : file = "{path}/{name}/{assembler}/mapping/maxbin/maxbin_{name}",
             maxbin_abunds = "{path}/{name}/{assembler}/mapping/maxbin/maxbin_{name}.tsv"
    threads : THREADS
    shell : """
    mkdir `dirname  {output.maxbin_abunds}`/abunds
    columns=`head -n1 {input.assembly} | tr "\t" "\n" | grep -n bam | grep -v var | cut -f1 -d":" | sed 's/$/,/' | tr -d '\n'`
    cut -f1,${{columns%%,}} -d$'\t' {input.mapping} > {output.maxbin_abunds}

    for f in `head -n1 {output.maxbin_abunds} | tr '\t' '\n'| grep -v contigName`
    do n=`head -n1 {output.maxbin_abunds} | tr "\t" "\n" | grep -n $f | cut -f1 -d":"`
    cut -f1,$n bla | grep -v contig > `basedir {output.maxbin_abunds}`/abunds/$f.tsv
    done

    ls `basedir {output.maxbin_abunds}`/abunds/$f.tsv > {output.maxbin_abunds}.lst


    {params.max_exec} -contig {input.assembly} -out {output.file} -abund_list {output.maxbin_abunds}.lst -thread {threads}
    """

rule phylophlan :
    params : phylophlan_path = "/home/moritz/repos/github/phylophlan",
             phylophlan_exe = "phylophlan.py",
             phylophlan_taxa = "data/ppafull.tax.txt",
             default_genomes = "/home/moritz/repos/github/phylophlan/input/default_genomes"
    input : path = "{path}/{name}/bins/{assembler}/{type}MAGs", annotated = "{path}/{name}/bins/{assembler}/annotated"
    output : "{path}/{name}/bins/{assembler}/{type}{name}.tree"
    threads : THREADS
    shell : """
    DD=`pwd`
    mkdir {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    cp {input.path}/*/*.faa {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    cd {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    for f in `ls *.faa`
    do
        sed -i 's/*//g' $f
    done
    cp {params.default_genomes}/*.faa .
    cd ../..
    python2.7 phylophlan.py --nproc {threads}  -i -t  {wildcards.type}{wildcards.name}
    IFS=$"\n"; for r in `cat data/ppafull.tax.txt`; do id=`echo ${{r}} | cut -f1`; tax=`echo ${{r}} | cut -f2`; sed -i "s/${{id}}/${{id}}_${{tax}}/g" output/{wildcards.type}{wildcards.name}/{wildcards.type}{wildcards.name}.tree.int.nwk; done; unset IFS
    cp output/{wildcards.type}{wildcards.name}/{wildcards.type}{wildcards.name}.tree.int.nwk $DD/{output}
    """


rule annotate_all_mags :
    input : "{path}/{name}/{assembler}/MAGs/metabat_{name}-unbinned.fa"
    output : "{path}/{name}/{assembler}/MAGs/metabat_{name}_unbinned/{name}_unbinned.checkm"
    threads : THREADS
    shell : """

    input_dir=`dirname {input}`

    for b in `ls $input_dir/*.fa`
    do
        bin_id=`basename $b | sed "s#metabat_{wildcards.name}-\\(.*\\).fa#\\1#"`
        out_dir=$input_dir/metabat_{wildcards.name}_$bin_id

        mkdir -p $out_dir

        if [ $bin_id = 'unbinned' ]
        then
            prokka --outdir /scratch/{wildcards.name}_$bin_id/  --metagenome --force --prefix {wildcards.name}_$bin_id --locustag {wildcards.name}_$bin_id --cpus {threads} $b
            mv /scratch/{wildcards.name}_$bin_id/* $out_dir/
            touch $out_dir/{wildcards.name}_$bin_id.checkm
        else
            prokka --outdir /scratch/{wildcards.name}_$bin_id/  --force --prefix {wildcards.name}_$bin_id --locustag {wildcards.name}_$bin_id --cpus {threads} $b
            mv /scratch/{wildcards.name}_$bin_id/* $out_dir/

            checkm lineage_wf -t {threads} -x fna $out_dir $out_dir/data > $out_dir/{wildcards.name}_$bin_id.checkm
            rm -r $out_dir/data
        fi
    done
    """


rule filter_all_good_MAGs :
# cutoff based on https://www.microbe.net/2017/12/13/why-genome-completeness-and-contamination-estimates-are-more-complicated-than-you-think/
    params : contamination = 5, completeness = 50000 #contamination = 5, completeness = 40
    input : []
    output : stats = "{path}/all_good_mags.stats", proteoms = "{path}/proteom/", genomes = "{path}/genomes/"
    run :
        from os.path import join as pjoin
        import os
        import shutil
        import numpy
        import pandas

        MAGstats = [y for x in tqdm(os.walk(".")) for y in glob(os.path.join(x[0], '*.magstats'))]
        big_dat = None
        for s in tqdm(MAGstats):
             dat = pandas.read_csv(s, index_col=0)
             if type(dat) != type(None):
                 folder = os.path.dirname(s)
                 name = "_".join(dat.index[0].split("_")[:-1])
                 dat['file'] = [pjoin(folder, "metabat" + "_" + f, f + ".faa") for f in dat.index]
                 dat = dat.loc[numpy.logical_and(dat.length > params.completeness , dat.contamination < params.contamination)]
                 if type(big_dat) != type(None):
                     big_dat = big_dat.append(dat)
                 else :
                     big_dat = dat
        big_dat['assmbler'] = [f.split("/")[-4] for f in list(big_dat.file)]
        big_dat['type'] = ["coass" if "1500_assemblies" == f.split("/")[-7] else "single" for f in list(big_dat.file)]
        big_dat['bin_name'] = [os.path.basename(f).replace(".faa","") for f in list(big_dat.file)]
	big_dat.index = [ a + "_" + b for a,b in zip(big_dat.assmbler, big_dat.bin_name)]
        big_dat.to_csv(output.stats)
        for v in tqdm(big_dat.to_dict(orient = "index").values()):
            shutil.copy(v['file'], pjoin(output.proteoms, v['type'] + "_" + v['assmbler'] + "_" + v['bin_name'] + ".faa" ))
            shutil.copy(v['file'].replace(".faa",".fna"), pjoin(output.genomes, v['type'] + "_" + v['assmbler'] + "_" + v['bin_name'] + ".fna" ))



rule library:
    input :           "{path}/{sample}/reads/mash/{sample}.msh",
            "{path}/{sample}/megahit/mapping/mapping_rates.txt",
            "{path}/{sample}/reads/kaiju/{sample}_kaiju.out.summary"
    output : "{path}/{sample}/done"
    shell:
        "touch {wildcards.path}/{wildcards.sample}/done"



rule all_libraries :
    input : all_dones ,
    output : "1000_processed_reads/done"
    shell : "touch 1000_processed_reads/done"
